name: 1. Scrape Websites

on:
  # Run every 6 hours
  schedule:
    - cron: '0 */6 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      company_filter:
        description: 'Company name filter (optional)'
        required: false
        type: string
      force_scrape:
        description: 'Force re-scrape even if no changes detected'
        required: false
        type: boolean
        default: false

env:
  NODE_ENV: production

jobs:
  scrape-websites:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    permissions:
      contents: write
      actions: read
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history to allow proper rebasing
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install dependencies
        run: |
          cd github-actions-backend
          # Use npm install instead of npm ci to handle package-lock.json mismatches
          npm install --omit=dev
      
      - name: Install Puppeteer browsers
        run: |
          cd github-actions-backend
          npx puppeteer browsers install chrome
      
      - name: Initialize or restore database
        run: |
          cd github-actions-backend
          
          # Check if database exists in repository
          if [ -f data/monitor.db ]; then
            echo "‚úÖ Database found in repository"
          else
            echo "üìä Database not found, initializing..."
            mkdir -p data
            node scripts/init-db.js
            echo "‚úÖ Database initialized with 52 companies"
          fi
      
      - name: Run intelligent scraper
        run: |
          cd github-actions-backend
          
          # Add company filter if provided
          if [ -n "${{ inputs.company_filter }}" ]; then
            echo "üéØ Filtering for company: ${{ inputs.company_filter }}"
            # TODO: Add company filter support to scraper
          fi
          
          # Run scraper
          node scraper-wrapper.js
        env:
          GITHUB_ACTIONS: 'true'
          FORCE_SCRAPE: ${{ inputs.force_scrape }}
      
      - name: Convert HTML to Markdown
        run: |
          cd github-actions-backend
          echo "üìù Converting HTML to Markdown..."
          node markdown-converter.js latest
      
      - name: Upload database backup
        uses: actions/upload-artifact@v4
        with:
          name: scraper-database-${{ github.run_id }}
          path: github-actions-backend/data/monitor.db
          retention-days: 7
      
      - name: Create scrape summary
        run: |
          cd github-actions-backend
          mkdir -p ../api-data
          echo "{
            \"workflow\": \"scrape\",
            \"run_id\": \"${{ github.run_id }}\",
            \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)\",
            \"status\": \"completed\"
          }" > ../api-data/last-scrape.json
      
      - name: Commit database changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Add the database and scrape status
          git add -f github-actions-backend/data/monitor.db || true
          git add api-data/last-scrape.json || true
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "üï∑Ô∏è Update scraped content - $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)"
            
            # Pull and rebase to avoid conflicts
            git pull --rebase origin main || true
            
            # Push changes
            git push origin main
          fi
      
      # Trigger next workflow if successful
      - name: Trigger analysis workflow
        if: success()
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'analyze.yml',
              ref: 'main',
              inputs: {
                trigger_source: 'scrape',
                run_id: '${{ github.run_id }}'
              }
            });
